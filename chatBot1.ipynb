{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u84I3FBNcvDk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWNUnJQHgXdr"
      },
      "outputs": [],
      "source": [
        "f = open(\"/content/data.txt\", 'r', errors= 'ignore')\n",
        "raw_data =f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r05l9NLThBVi",
        "outputId": "f9d759e6-9fbb-4da5-da41-c455b80eb69d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data = raw_data.lower()\n",
        "nltk.download('punkt')#tokenizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrIaWiUBkCij"
      },
      "outputs": [],
      "source": [
        "sentence_token = nltk.sent_tokenize(raw_data)\n",
        "word_token = nltk.word_tokenize(raw_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "y3VnhGvniqDp",
        "outputId": "3053fc34-254e-41b3-bd1b-905458c090ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6103bcc54bea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sentence_token' is not defined"
          ]
        }
      ],
      "source": [
        "sentence_token[:1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt491Ugkkck-",
        "outputId": "f9032cb1-9783-4e47-9e6a-045221094291"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['do',\n",
              " 'you',\n",
              " 'love',\n",
              " 'me',\n",
              " ':',\n",
              " '.',\n",
              " 'i',\n",
              " 'love',\n",
              " 'you',\n",
              " '.',\n",
              " 'listen',\n",
              " 'to',\n",
              " 'this',\n",
              " 'article',\n",
              " 'from',\n",
              " 'wikipedia',\n",
              " ',',\n",
              " 'the',\n",
              " 'free',\n",
              " 'encyclopedia',\n",
              " 'for',\n",
              " 'other',\n",
              " 'uses',\n",
              " ',',\n",
              " 'see',\n",
              " 'universe',\n",
              " '(',\n",
              " 'disambiguation',\n",
              " ')',\n",
              " '.',\n",
              " 'universe',\n",
              " 'are',\n",
              " 'you',\n",
              " 'human',\n",
              " '.',\n",
              " 'are',\n",
              " 'you',\n",
              " 'a',\n",
              " 'robot',\n",
              " '.',\n",
              " 'what',\n",
              " 'is',\n",
              " 'your',\n",
              " 'name',\n",
              " '.',\n",
              " 'reterival',\n",
              " 'chatbot',\n",
              " '.',\n",
              " 'how',\n",
              " 'old',\n",
              " 'are',\n",
              " 'you',\n",
              " '.',\n",
              " 'what',\n",
              " '’',\n",
              " 's',\n",
              " 'your',\n",
              " 'age',\n",
              " 'what',\n",
              " 'day',\n",
              " 'is',\n",
              " 'it',\n",
              " 'today',\n",
              " 'what',\n",
              " 'do',\n",
              " 'you',\n",
              " 'do',\n",
              " 'with',\n",
              " 'my',\n",
              " 'data',\n",
              " '?',\n",
              " '/',\n",
              " 'do',\n",
              " 'you',\n",
              " 'save',\n",
              " 'what',\n",
              " 'i',\n",
              " 'say',\n",
              " '?',\n",
              " 'who',\n",
              " 'made',\n",
              " 'you',\n",
              " '.',\n",
              " 'not',\n",
              " 'important',\n",
              " '.',\n",
              " 'which',\n",
              " 'languages',\n",
              " 'can',\n",
              " 'you',\n",
              " 'speak',\n",
              " '.',\n",
              " 'what',\n",
              " 'is',\n",
              " 'your',\n",
              " 'mother',\n",
              " '’',\n",
              " 's',\n",
              " 'name',\n",
              " '.',\n",
              " 'i',\n",
              " 'am',\n",
              " 'robort',\n",
              " '.',\n",
              " 'where',\n",
              " 'do',\n",
              " 'you',\n",
              " 'live',\n",
              " ':',\n",
              " 'hearts',\n",
              " '.',\n",
              " 'how',\n",
              " 'many',\n",
              " 'people',\n",
              " 'can',\n",
              " 'you',\n",
              " 'speak',\n",
              " 'to',\n",
              " 'at',\n",
              " 'once',\n",
              " '?',\n",
              " 'what',\n",
              " '’',\n",
              " 's',\n",
              " 'the',\n",
              " 'weather',\n",
              " 'like',\n",
              " 'today',\n",
              " '?',\n",
              " 'do',\n",
              " 'you',\n",
              " 'have',\n",
              " 'a',\n",
              " 'job',\n",
              " 'for',\n",
              " 'me',\n",
              " '?',\n",
              " '/',\n",
              " 'where',\n",
              " 'can',\n",
              " 'i',\n",
              " 'apply',\n",
              " '?',\n",
              " 'are',\n",
              " 'you',\n",
              " 'expensive',\n",
              " '?',\n",
              " 'who',\n",
              " '’',\n",
              " 's',\n",
              " 'your',\n",
              " 'boss',\n",
              " '.',\n",
              " 'i',\n",
              " 'am',\n",
              " 'boss',\n",
              " '.',\n",
              " 'do',\n",
              " 'you',\n",
              " 'get',\n",
              " 'smarter',\n",
              " '.',\n",
              " 'yessss',\n",
              " '.',\n",
              " 'hubble',\n",
              " 'ultra',\n",
              " 'deep',\n",
              " 'field.jpg',\n",
              " 'the',\n",
              " 'hubble',\n",
              " 'ultra-deep',\n",
              " 'field',\n",
              " 'image',\n",
              " 'shows',\n",
              " 'some',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'remote',\n",
              " 'galaxies',\n",
              " 'visible',\n",
              " 'to',\n",
              " 'present',\n",
              " 'technology',\n",
              " '(',\n",
              " 'diagonal',\n",
              " 'is',\n",
              " '~1/10',\n",
              " 'apparent',\n",
              " 'moon',\n",
              " 'diameter',\n",
              " ')',\n",
              " '[',\n",
              " '1',\n",
              " ']',\n",
              " 'age',\n",
              " '(',\n",
              " 'within',\n",
              " 'λcdm',\n",
              " 'model',\n",
              " ')',\n",
              " '13.787',\n",
              " '±',\n",
              " '0.020',\n",
              " 'billion',\n",
              " 'years',\n",
              " '[',\n",
              " '2',\n",
              " ']',\n",
              " 'diameter',\n",
              " 'unknown',\n",
              " '.',\n",
              " '[',\n",
              " '3',\n",
              " ']',\n",
              " 'observable',\n",
              " 'universe',\n",
              " ':',\n",
              " '8.8×1026',\n",
              " 'm',\n",
              " '(',\n",
              " '28.5',\n",
              " 'gpc',\n",
              " 'or',\n",
              " '93',\n",
              " 'gly',\n",
              " ')',\n",
              " '[',\n",
              " '4',\n",
              " ']',\n",
              " 'mass',\n",
              " '(',\n",
              " 'ordinary',\n",
              " 'matter',\n",
              " ')',\n",
              " 'at',\n",
              " 'least',\n",
              " '1053',\n",
              " 'kg',\n",
              " '[',\n",
              " '5',\n",
              " ']',\n",
              " 'average',\n",
              " 'density',\n",
              " '(',\n",
              " 'with',\n",
              " 'energy',\n",
              " ')',\n",
              " '9.9×10−27',\n",
              " 'kg/m3',\n",
              " '[',\n",
              " '6',\n",
              " ']',\n",
              " 'average',\n",
              " 'temperature',\n",
              " '2.72548',\n",
              " 'k',\n",
              " '(',\n",
              " '−270.4',\n",
              " '°c',\n",
              " ',',\n",
              " '−454.8',\n",
              " '°f',\n",
              " ')',\n",
              " '[',\n",
              " '7',\n",
              " ']',\n",
              " 'main',\n",
              " 'contents',\n",
              " 'ordinary',\n",
              " '(',\n",
              " 'baryonic',\n",
              " ')',\n",
              " 'matter',\n",
              " '(',\n",
              " '4.9',\n",
              " '%',\n",
              " ')',\n",
              " 'dark',\n",
              " 'matter',\n",
              " '(',\n",
              " '26.8',\n",
              " '%',\n",
              " ')',\n",
              " 'dark',\n",
              " 'energy',\n",
              " '(',\n",
              " '68.3',\n",
              " '%',\n",
              " ')',\n",
              " '[',\n",
              " '8',\n",
              " ']',\n",
              " 'shape',\n",
              " 'flat',\n",
              " 'with',\n",
              " '4‰',\n",
              " 'error',\n",
              " 'margin',\n",
              " '[',\n",
              " '9',\n",
              " ']',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'is',\n",
              " 'all',\n",
              " 'of',\n",
              " 'space',\n",
              " 'and',\n",
              " 'time',\n",
              " '[',\n",
              " 'a',\n",
              " ']',\n",
              " 'and',\n",
              " 'their',\n",
              " 'contents',\n",
              " ',',\n",
              " '[',\n",
              " '10',\n",
              " ']',\n",
              " 'including',\n",
              " 'planets',\n",
              " ',',\n",
              " 'stars',\n",
              " ',',\n",
              " 'galaxies',\n",
              " ',',\n",
              " 'and',\n",
              " 'all',\n",
              " 'other',\n",
              " 'forms',\n",
              " 'of',\n",
              " 'matter',\n",
              " 'and',\n",
              " 'energy',\n",
              " '.',\n",
              " 'the',\n",
              " 'big',\n",
              " 'bang',\n",
              " 'theory',\n",
              " 'is',\n",
              " 'the',\n",
              " 'prevailing',\n",
              " 'cosmological',\n",
              " 'description',\n",
              " 'of',\n",
              " 'the',\n",
              " 'development',\n",
              " 'of',\n",
              " 'the',\n",
              " 'universe',\n",
              " '.',\n",
              " 'according',\n",
              " 'to',\n",
              " 'this',\n",
              " 'theory',\n",
              " ',',\n",
              " 'space',\n",
              " 'and',\n",
              " 'time',\n",
              " 'emerged',\n",
              " 'together',\n",
              " '13.787±0.020',\n",
              " 'billion',\n",
              " 'years',\n",
              " 'ago',\n",
              " ',',\n",
              " '[',\n",
              " '11',\n",
              " ']',\n",
              " 'and',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'has',\n",
              " 'been',\n",
              " 'expanding',\n",
              " 'ever',\n",
              " 'since',\n",
              " 'the',\n",
              " 'big',\n",
              " 'bang',\n",
              " '.',\n",
              " 'while',\n",
              " 'the',\n",
              " 'spatial',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'universe',\n",
              " 'is',\n",
              " 'unknown',\n",
              " ',',\n",
              " '[',\n",
              " '3',\n",
              " ']',\n",
              " 'it',\n",
              " 'is',\n",
              " 'possible',\n",
              " 'to',\n",
              " 'measure',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'observable',\n",
              " 'universe',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'approximately',\n",
              " '93',\n",
              " 'billion',\n",
              " 'light-years',\n",
              " 'in',\n",
              " 'diameter',\n",
              " 'at',\n",
              " 'the',\n",
              " 'present',\n",
              " 'day',\n",
              " '.',\n",
              " 'some',\n",
              " 'of',\n",
              " 'the',\n",
              " 'earliest',\n",
              " 'cosmological',\n",
              " 'models',\n",
              " 'of',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'were',\n",
              " 'developed',\n",
              " 'by',\n",
              " 'ancient',\n",
              " 'greek',\n",
              " 'and',\n",
              " 'indian',\n",
              " 'philosophers',\n",
              " 'and',\n",
              " 'were',\n",
              " 'geocentric',\n",
              " ',',\n",
              " 'placing',\n",
              " 'earth',\n",
              " 'at',\n",
              " 'the',\n",
              " 'center',\n",
              " '.',\n",
              " '[',\n",
              " '12',\n",
              " ']',\n",
              " '[',\n",
              " '13',\n",
              " ']',\n",
              " 'over',\n",
              " 'the',\n",
              " 'centuries',\n",
              " ',',\n",
              " 'more',\n",
              " 'precise',\n",
              " 'astronomical',\n",
              " 'observations',\n",
              " 'led',\n",
              " 'nicolaus',\n",
              " 'copernicus',\n",
              " 'to',\n",
              " 'develop',\n",
              " 'the',\n",
              " 'heliocentric',\n",
              " 'model',\n",
              " 'with',\n",
              " 'the',\n",
              " 'sun',\n",
              " 'at',\n",
              " 'the',\n",
              " 'center',\n",
              " 'of',\n",
              " 'the',\n",
              " 'solar',\n",
              " 'system',\n",
              " '.',\n",
              " 'in',\n",
              " 'developing',\n",
              " 'the',\n",
              " 'law',\n",
              " 'of',\n",
              " 'universal',\n",
              " 'gravitation',\n",
              " ',',\n",
              " 'isaac',\n",
              " 'newton',\n",
              " 'built',\n",
              " 'upon',\n",
              " 'copernicus',\n",
              " \"'s\",\n",
              " 'work',\n",
              " 'as',\n",
              " 'well',\n",
              " 'as',\n",
              " 'johannes',\n",
              " 'kepler',\n",
              " \"'s\",\n",
              " 'laws',\n",
              " 'of',\n",
              " 'planetary',\n",
              " 'motion',\n",
              " 'and',\n",
              " 'observations',\n",
              " 'by',\n",
              " 'tycho',\n",
              " 'brahe',\n",
              " '.',\n",
              " 'further',\n",
              " 'observational',\n",
              " 'improvements',\n",
              " 'led',\n",
              " 'to',\n",
              " 'the',\n",
              " 'realization',\n",
              " 'that',\n",
              " 'the',\n",
              " 'sun',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'a',\n",
              " 'few',\n",
              " 'hundred',\n",
              " 'billion',\n",
              " 'stars',\n",
              " 'in',\n",
              " 'the',\n",
              " 'milky',\n",
              " 'way',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'a',\n",
              " 'few',\n",
              " 'hundred',\n",
              " 'billion',\n",
              " 'galaxies',\n",
              " 'in',\n",
              " 'the',\n",
              " 'observable',\n",
              " 'universe',\n",
              " '.',\n",
              " 'many',\n",
              " 'of',\n",
              " 'the',\n",
              " 'stars',\n",
              " 'in',\n",
              " 'a',\n",
              " 'galaxy',\n",
              " 'have',\n",
              " 'planets',\n",
              " '.',\n",
              " 'at',\n",
              " 'the',\n",
              " 'largest',\n",
              " 'scale',\n",
              " ',',\n",
              " 'galaxies',\n",
              " 'are',\n",
              " 'distributed',\n",
              " 'uniformly',\n",
              " 'and',\n",
              " 'the',\n",
              " 'same',\n",
              " 'in',\n",
              " 'all',\n",
              " 'directions',\n",
              " ',',\n",
              " 'meaning',\n",
              " 'that',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'has',\n",
              " 'neither',\n",
              " 'an',\n",
              " 'edge',\n",
              " 'nor',\n",
              " 'a',\n",
              " 'center',\n",
              " '.',\n",
              " 'at',\n",
              " 'smaller',\n",
              " 'scales',\n",
              " ',',\n",
              " 'galaxies',\n",
              " 'are',\n",
              " 'distributed',\n",
              " 'in',\n",
              " 'clusters',\n",
              " 'and',\n",
              " 'superclusters',\n",
              " 'which',\n",
              " 'form',\n",
              " 'immense',\n",
              " 'filaments',\n",
              " 'and',\n",
              " 'voids',\n",
              " 'in',\n",
              " 'space',\n",
              " ',',\n",
              " 'creating',\n",
              " 'a',\n",
              " 'vast',\n",
              " 'foam-like',\n",
              " 'structure',\n",
              " '.',\n",
              " '[',\n",
              " '14',\n",
              " ']',\n",
              " 'discoveries',\n",
              " 'in',\n",
              " 'the',\n",
              " 'early',\n",
              " '20th',\n",
              " 'century',\n",
              " 'have',\n",
              " 'suggested',\n",
              " 'that',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'had',\n",
              " 'a',\n",
              " 'beginning',\n",
              " 'and',\n",
              " 'that',\n",
              " 'space',\n",
              " 'has',\n",
              " 'been',\n",
              " 'expanding',\n",
              " 'since',\n",
              " 'then',\n",
              " '[',\n",
              " '15',\n",
              " ']',\n",
              " 'at',\n",
              " 'an',\n",
              " 'increasing',\n",
              " 'rate',\n",
              " '.',\n",
              " '[',\n",
              " '16',\n",
              " ']',\n",
              " 'according',\n",
              " 'to',\n",
              " 'the',\n",
              " 'big',\n",
              " 'bang',\n",
              " 'theory',\n",
              " ',',\n",
              " 'the',\n",
              " 'energy',\n",
              " 'and',\n",
              " 'matter',\n",
              " 'initially',\n",
              " 'present',\n",
              " 'have',\n",
              " 'become',\n",
              " 'less',\n",
              " 'dense',\n",
              " 'as',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'expanded',\n",
              " '.',\n",
              " 'after',\n",
              " 'an',\n",
              " 'initial',\n",
              " 'accelerated',\n",
              " 'expansion',\n",
              " 'called',\n",
              " 'the',\n",
              " 'inflationary',\n",
              " 'epoch',\n",
              " 'at',\n",
              " 'around',\n",
              " '10−32',\n",
              " 'seconds',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'separation',\n",
              " 'of',\n",
              " 'the',\n",
              " 'four',\n",
              " 'known',\n",
              " 'fundamental',\n",
              " 'forces',\n",
              " ',',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'gradually',\n",
              " 'cooled',\n",
              " 'and',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'expand',\n",
              " ',',\n",
              " 'allowing',\n",
              " 'the',\n",
              " 'first',\n",
              " 'subatomic',\n",
              " 'particles',\n",
              " 'and',\n",
              " 'simple',\n",
              " 'atoms',\n",
              " 'to',\n",
              " 'form',\n",
              " '.',\n",
              " 'dark',\n",
              " 'matter',\n",
              " 'gradually',\n",
              " 'gathered',\n",
              " ',',\n",
              " 'forming',\n",
              " 'a',\n",
              " 'foam-like',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'filaments',\n",
              " 'and',\n",
              " 'voids',\n",
              " 'under',\n",
              " 'the',\n",
              " 'influence',\n",
              " 'of',\n",
              " 'gravity',\n",
              " '.',\n",
              " 'giant',\n",
              " 'clouds',\n",
              " 'of',\n",
              " 'hydrogen',\n",
              " 'and',\n",
              " 'helium',\n",
              " 'were',\n",
              " 'gradually',\n",
              " 'drawn',\n",
              " 'to',\n",
              " 'the',\n",
              " 'places',\n",
              " 'where',\n",
              " 'dark',\n",
              " 'matter',\n",
              " 'was',\n",
              " 'most',\n",
              " 'dense',\n",
              " ',',\n",
              " 'forming',\n",
              " 'the',\n",
              " 'first',\n",
              " 'galaxies',\n",
              " ',',\n",
              " 'stars',\n",
              " ',',\n",
              " 'and',\n",
              " 'everything',\n",
              " 'else',\n",
              " 'seen',\n",
              " 'today',\n",
              " '.',\n",
              " 'from',\n",
              " 'studying',\n",
              " 'the',\n",
              " 'movement',\n",
              " 'of',\n",
              " 'galaxies',\n",
              " ',',\n",
              " 'it',\n",
              " 'has',\n",
              " 'been',\n",
              " 'discovered',\n",
              " 'that',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'contains',\n",
              " 'much',\n",
              " 'more',\n",
              " 'matter',\n",
              " 'than',\n",
              " 'is',\n",
              " 'accounted',\n",
              " 'for',\n",
              " 'by',\n",
              " 'visible',\n",
              " 'objects',\n",
              " ';',\n",
              " 'stars',\n",
              " ',',\n",
              " 'galaxies',\n",
              " ',',\n",
              " 'nebulas',\n",
              " 'and',\n",
              " 'interstellar',\n",
              " 'gas',\n",
              " '.',\n",
              " 'this',\n",
              " 'unseen',\n",
              " 'matter',\n",
              " 'is',\n",
              " 'known',\n",
              " 'as',\n",
              " 'dark',\n",
              " 'matter',\n",
              " '[',\n",
              " '17',\n",
              " ']',\n",
              " '(',\n",
              " 'dark',\n",
              " 'means',\n",
              " 'that',\n",
              " 'there',\n",
              " 'is',\n",
              " 'a',\n",
              " 'wide',\n",
              " 'range',\n",
              " 'of',\n",
              " 'strong',\n",
              " 'indirect',\n",
              " 'evidence',\n",
              " 'that',\n",
              " 'it',\n",
              " 'exists',\n",
              " ',',\n",
              " 'but',\n",
              " 'we',\n",
              " 'have',\n",
              " 'not',\n",
              " 'yet',\n",
              " 'detected',\n",
              " 'it',\n",
              " 'directly',\n",
              " ')',\n",
              " '.',\n",
              " 'the',\n",
              " 'λcdm',\n",
              " 'model',\n",
              " 'is',\n",
              " 'the',\n",
              " 'most',\n",
              " 'widely',\n",
              " 'accepted',\n",
              " 'model',\n",
              " 'of',\n",
              " 'the',\n",
              " 'universe',\n",
              " '.',\n",
              " 'it',\n",
              " 'suggests',\n",
              " 'that',\n",
              " 'about',\n",
              " '69.2',\n",
              " '%',\n",
              " '±1.2',\n",
              " '%',\n",
              " 'of',\n",
              " 'the',\n",
              " 'mass',\n",
              " 'and',\n",
              " 'energy',\n",
              " 'in',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'is',\n",
              " 'dark',\n",
              " 'energy',\n",
              " 'which',\n",
              " 'is',\n",
              " 'responsible',\n",
              " 'for',\n",
              " 'the',\n",
              " 'acceleration',\n",
              " 'of',\n",
              " 'the',\n",
              " 'expansion',\n",
              " 'of',\n",
              " 'space',\n",
              " ',',\n",
              " 'and',\n",
              " 'about',\n",
              " '25.8',\n",
              " '%',\n",
              " '±1.1',\n",
              " '%',\n",
              " 'is',\n",
              " 'dark',\n",
              " 'matter',\n",
              " '.',\n",
              " '[',\n",
              " '18',\n",
              " ']',\n",
              " 'ordinary',\n",
              " '(',\n",
              " \"'baryonic\",\n",
              " \"'\",\n",
              " ')',\n",
              " 'matter',\n",
              " 'is',\n",
              " 'therefore',\n",
              " 'only',\n",
              " '4.84',\n",
              " '%',\n",
              " '±0.1',\n",
              " '%',\n",
              " 'of',\n",
              " 'the',\n",
              " 'physical',\n",
              " 'universe',\n",
              " '.',\n",
              " '[',\n",
              " '18',\n",
              " ']',\n",
              " 'stars',\n",
              " ',',\n",
              " 'planets',\n",
              " ',',\n",
              " 'and',\n",
              " 'visible',\n",
              " 'gas',\n",
              " 'clouds',\n",
              " 'only',\n",
              " 'form',\n",
              " 'about',\n",
              " '6',\n",
              " '%',\n",
              " 'of',\n",
              " 'the',\n",
              " 'ordinary',\n",
              " 'matter',\n",
              " '.',\n",
              " '[',\n",
              " '19',\n",
              " ']',\n",
              " 'there',\n",
              " 'are',\n",
              " 'many',\n",
              " 'competing',\n",
              " 'hypotheses',\n",
              " 'about',\n",
              " 'the',\n",
              " 'ultimate',\n",
              " 'fate',\n",
              " 'of',\n",
              " 'the',\n",
              " 'universe',\n",
              " 'and',\n",
              " 'about',\n",
              " 'what',\n",
              " ',',\n",
              " 'if',\n",
              " 'anything',\n",
              " ',',\n",
              " 'preceded',\n",
              " 'the',\n",
              " 'big',\n",
              " 'bang',\n",
              " ',',\n",
              " 'while',\n",
              " 'other',\n",
              " 'physicists',\n",
              " 'and',\n",
              " 'philosophers',\n",
              " 'refuse',\n",
              " 'to',\n",
              " 'speculate',\n",
              " ',',\n",
              " 'doubting',\n",
              " 'that',\n",
              " 'information',\n",
              " 'about',\n",
              " 'prior',\n",
              " 'states',\n",
              " 'will',\n",
              " 'ever',\n",
              " 'be',\n",
              " 'accessible',\n",
              " '.',\n",
              " 'some',\n",
              " 'physicists',\n",
              " 'have',\n",
              " 'suggested',\n",
              " 'various',\n",
              " 'multiverse',\n",
              " 'hypotheses',\n",
              " ',',\n",
              " 'in',\n",
              " 'which',\n",
              " 'our',\n",
              " 'universe',\n",
              " 'might',\n",
              " 'be',\n",
              " 'one',\n",
              " 'among',\n",
              " 'many',\n",
              " 'universes',\n",
              " ...]"
            ]
          },
          "execution_count": 287,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XQe9CTW96UO"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def  LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_puct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "def lemNormalizer(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_puct_dict)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN1dKgDVkhhH"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTqt_uM5IEJd"
      },
      "outputs": [],
      "source": [
        "greet_input = ('hey','hi','hii','there','hello')\n",
        "greet_reponse= ('suuupp','there there','missed you','lets start')\n",
        "def greet(sentence):\n",
        "  for words in sentence.split():\n",
        "    if words == greet_input:\n",
        "        return random.choice(greet_reponse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c7VimXkQwTp"
      },
      "outputs": [],
      "source": [
        "def reponse(user_response):\n",
        "  robo_respone = ''\n",
        "  tfidfVec = TfidfVectorizer(tokenizer=lemNormalizer,stop_words='english')\n",
        "  tfidf = tfidfVec.fit_transform(sentence_token)\n",
        "  val = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = val.argsort()[0][-2]\n",
        "  flats = val.flatten()\n",
        "  flats.sort()\n",
        "  req_tfidt = flats[-2]\n",
        "  if req_tfidt == 0:\n",
        "    robo_respone = robo_respone+'Sorry!! Unable to understand you'\n",
        "    return robo_respone\n",
        "  else:\n",
        "    robo_respone = robo_respone+sentence_token[idx]\n",
        "    return robo_respone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcF8LXo4-riv",
        "outputId": "872a1f76-93d8-49d9-f179-4b7e0d693146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helooo!! i am retreival learning bot! Start talking with me . For ending canvo type bye....!\n",
            "Bot:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"barbara mcclintock: a brief biographical sketch\".\n",
            "Bot:\"barbara mcclintock: a brief biographical sketch\".\n",
            "Bot:[127]\n",
            "\n",
            "in science\n",
            "further information: model organism\n",
            "\n",
            "barbara mcclintock used maize to study inheritance of traits.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "flag = True\n",
        "print(\"Helooo!! i am retreival learning bot! Start talking with me . For ending canvo type bye....!\")\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if user_response != 'bye':\n",
        "    if user_response=='thanks' or user_response=='thank you':\n",
        "      flag = False\n",
        "      print('Bot: u r welcome.....!')\n",
        "    else:\n",
        "      if (greet(user_response) != None):\n",
        "        print('Bot:'+ greet(user_response))\n",
        "      else:\n",
        "        sentence_token.append(user_response)\n",
        "        word_token = word_token + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_token))\n",
        "        print('Bot:', end = '')\n",
        "        print(reponse(user_response))\n",
        "        sentence_token.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print('Goodbye.....!!')\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}